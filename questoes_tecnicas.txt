1) Qual o objetivo do comando cache em Spark?
Persiste um rdd na memoria. Caso seja um dataset, será persistido em memória e no disco. O objetivo é fazer com que o dataset ou o rdd seja de rápido pelas funções ou códigos do Spark já que as operações são executadas em lazy evaluation;

2) O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?
As operações em MapReduce trabalham com escritas constantes em disco, enquanto o Spark trabalha em memória

3) Qual é a função do SparkContext?
É a porta de entrada para todas as funções spark, a conexão com o cluster Spark

4) Explique com suas palavras o que é Resilient Distributed Datasets (RDD)
Uma coleção de objetos distribuida e imutavel, utilizada para armazenar dados de todos os tipos, estruturados ou não.

5) GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?
Enquanto o GroupByKey embaralha os dados e os envia para o processamento no Cluster, o reduceByKey() os pares do dataset
são combinados antes que os dados sejam embaralhados e enviados para o cluster

6) Explique o que o código Scala abaixo faz.
val textFile = sc.textFile ( "hdfs://..." )
val counts = textFile.flatMap(line => line.split( " " )).map( word => ( word , 1 )).reduceByKey( _ + _ )
counts.saveAsTextFile ( "hdfs://..." )

# file ("hello world hello world")
# counts (flatMap) -> rdd(["hello", "world", "hello", "world"])
# counts (map) -> rdd([("hello", 1), ("world", 1), ("hello", 1), ("world", 1)]
# counts (reduceByKey) -> rdd([("hello", 2), ("world", 2)])


Lê um arquivo no hdfs
separa as linhas por " " (espaço) e armazena em um RDD contendo uma lista com cada palavra do arquivo no HDFS
Para cada palavra, cria uma tupla contendo a palavra e o número 1, retorna um RDD do mesmo tamanho do anterior
Agrupa os arquivos pelas chaves (indice 0 da tupla) e soma os valores das chaves
Salva o resultado em um arquivo de texto